### Note:
These notes were copied from obsidian notebook. Therefore, the spacing on the .md file may be weird. Please view the rendered version instead.

### Date Written: Aug 2025

#### What test I ran to check this info:
- 2 linear layers nn.Linear(20000, 20000) with 1 nn.Linear(20000, 1)
- pl.trainer(devices=1, strategy="ddp")


#### P.L. Trainer runtime log (key events only) (subjective)

| File_Name                              | Line#                            | Line Info                                                                                                                                                                                                                                                                                                                                                                                                             | What it does                                                                                                                                                                                                                                                                                                                                                                               |
| -------------------------------------- | -------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| my_file.py                             | -                                | trainer.fit()                                                                                                                                                                                                                                                                                                                                                                                                         | Everything is done here                                                                                                                                                                                                                                                                                                                                                                    |
| trainer.py                             |                                  | def fit(...):<br>    self.\_fit_impl                                                                                                                                                                                                                                                                                                                                                                                  | Main use of .fit() is to call fit implementation below                                                                                                                                                                                                                                                                                                                                     |
|                                        | 565                              | def \_fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)                                                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 586                              | self.\_data_connector.attach_data (model, train_loader, val_loader, datamodule)                                                                                                                                                                                                                                                                                                                                       | Links train & val dataloaders to the trainer object                                                                                                                                                                                                                                                                                                                                        |
|                                        | 593                              | self.\_checkpoint_connector.\_select_ckpt_path (self.state.fn, ckpt_path, model_provided= True, model_connected= self.lightning_module is not None)                                                                                                                                                                                                                                                                   | Sets up the model checkpointing path                                                                                                                                                                                                                                                                                                                                                       |
|                                        | 599                              | self.\_run(model, ckpt_path)                                                                                                                                                                                                                                                                                                                                                                                          | calls the run function                                                                                                                                                                                                                                                                                                                                                                     |
|                                        | 957                              | self.strategy.connect(model)                                                                                                                                                                                                                                                                                                                                                                                          | Loads your model into the strategy object. This is handled in strategy.py line 111                                                                                                                                                                                                                                                                                                         |
|                                        | 959<br>960                       | self.\_callback_connector.\_attach \_model_callbacks()<br>self.\_callback_connector.\_attach \_model_logging_functions()                                                                                                                                                                                                                                                                                              | Uses hooks to call configure_callback function etc.                                                                                                                                                                                                                                                                                                                                        |
|                                        | 968                              | self.strategy.setup_environment()                                                                                                                                                                                                                                                                                                                                                                                     | Sets up the cuda device. Sets up the distributed devices.                                                                                                                                                                                                                                                                                                                                  |
| ddp.py                                 | **152**<br>153                   | **def setup_environment(self):**<br>    super().setup_environment()                                                                                                                                                                                                                                                                                                                                                   | Calls functions below.<br>Continued 3 lines down.                                                                                                                                                                                                                                                                                                                                          |
| strategy.py                            | 129                              | self.accelerator.setup_device(self.root_device)                                                                                                                                                                                                                                                                                                                                                                       | Calls the function below                                                                                                                                                                                                                                                                                                                                                                   |
| cuda.py                                | 47                               | torch.cuda.set_device(device)                                                                                                                                                                                                                                                                                                                                                                                         | Sets up cuda GPU. This is only performed on the root device at this stage. This is required for spawning a process on the gpu.                                                                                                                                                                                                                                                             |
| ddp.py                                 | 154                              | self.setup_distributed()                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 197                              | def setup_distributed(self):<br>    reset_seed()<br>	self.set_world_ranks()<br>	self.process_group_backend = self.\_get_process_group_backend()<br>	\_init_dist_connection(self.cluster_environment, self.\_process_group_backend, self.timeout)                                                                                                                                                                      | Proccess group backend = nccl for cuda<br>\_init_dist_connection sets up the MASTER_ADDR and MASTER_PORT for syncing from the os.environ<br>==This is what enables distributed computing== view [pytorch docs link](https://docs.pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html) for more info. <br>At exit, this connection is also manually destroyed. |
|                                        |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                            |
| trainer.py                             | 969                              | self.\_\_setup_profiler()                                                                                                                                                                                                                                                                                                                                                                                             | Sets up the process on the GPU                                                                                                                                                                                                                                                                                                                                                             |
|                                        | 1101                             | self.profiler.\_lightning_module = proxy(self.lightning_module)                                                                                                                                                                                                                                                                                                                                                       | Creates a weakreference proxy to the lightning module.                                                                                                                                                                                                                                                                                                                                     |
|                                        | 1102                             | self.profiler.setup(stage=self.state.fn, local_rank, self.log_dir)                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 1244                             | def log_dir(self):<br>    ....<br>	dir_path = self.strategy.broadcast(dir_path)                                                                                                                                                                                                                                                                                                                                       | This sets the log directory on all ranks. It must be set on all processes otherwise the "program will stall forever".                                                                                                                                                                                                                                                                      |
| ddp.py                                 | 302                              | def broadcast(self, obj: TBroadcast, src):<br>    obj = \[obj\]<br>	torch.distributed.broadcast_object_list(obj, src, group=\_group.WORLD)                                                                                                                                                                                                                                                                            | Broadcasts picklable objects in object_list (obj here) to the whole group. This moves the cpu process into every GPU process. This is handled in pytorch's c10d_logger.py (sets up the broadcasting of a logger process)                                                                                                                                                                   |
|                                        |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 988                              | self.strategy.setup(self)                                                                                                                                                                                                                                                                                                                                                                                             | Strategy configures the model and moves it to the device. ==A complete run added 6.83Gb to GPU.== I believe this is caused by all model params being moved onto GPU                                                                                                                                                                                                                        |
| **ddp.py**                             | **157**                          | **def setup(self, trainer)**                                                                                                                                                                                                                                                                                                                                                                                          | **Handles setting up training (and adding things to gpu)**                                                                                                                                                                                                                                                                                                                                 |
|                                        | 167                              | self.model_to_device()                                                                                                                                                                                                                                                                                                                                                                                                | Moves model params to the device. ==Took 3.85Gb for me.==                                                                                                                                                                                                                                                                                                                                  |
|                                        | 171                              | self.configure_ddp()                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 280<br>283<br>284                | **def configure_ddp(self) -> None:**<br>    self.model = self.\_setup_model(self.model)<br>	self.\_register_ddp_hooks()                                                                                                                                                                                                                                                                                               | \_setup_model (self.model) in total added ==5.97Gb== making the total ==9.82Gb==                                                                                                                                                                                                                                                                                                           |
|                                        | 188<br>-<br>195                  | **def \_setup_model(self, model: Module):**<br>""" <br>Wraps the model into a :class: ~torch.nn.parallel.distributed.DistributedDataParallel module.<br>"""<br>    device_ids = self.determine_ddp_device_ids()<br>    ctx = torch.cuda.stream (torch.cuda.Stream()) if device_ids is not None else nullcontext()<br>with ctx:<br>    return DistributedDataParallel(module=model, device_ids, \*\*self.\_ddp_kwargs) | Relevant materials from pytorch: # https://pytorch.org/docs/stable/notes/cuda.html#id5<br><br>This is what causes the jump in GPU memory                                                                                                                                                                                                                                                   |
|                                        | 172                              | self.setup_optimizers(trainer)                                                                                                                                                                                                                                                                                                                                                                                        | Registers optimizers and validates them.                                                                                                                                                                                                                                                                                                                                                   |
| ddp.py                                 | 166<br><br>167                   | self.precision_plugin.convert_module (self.model)<br>self.model_to_device()                                                                                                                                                                                                                                                                                                                                           | Converts model to correct precision and loads it onto gpu.                                                                                                                                                                                                                                                                                                                                 |
|                                        | 180                              | \_optimizers_to_device(self.optimizers, self.root_device)                                                                                                                                                                                                                                                                                                                                                             | Moves optimizer to the device                                                                                                                                                                                                                                                                                                                                                              |
| light_fabric> utilities> apply_func.py | 78                               | def move_data_to_device(batch, device)                                                                                                                                                                                                                                                                                                                                                                                | What handles moving data to the device. This also applies to optimizer parameters.                                                                                                                                                                                                                                                                                                         |
| trainer.py                             | 1005                             | self.\_checkpoint_connector.resume_end()                                                                                                                                                                                                                                                                                                                                                                              | Signal the connector that all states have resumed and memory for the checkpoint object can be released. This occurs before any training even occurs.                                                                                                                                                                                                                                       |
|                                        |                                  | def resume_end(self):<br>    ...<br>	self.\_loaded_checkpoint = {}<br>	torch.cuda.empty_cache()<br>	self.trainer.strategy.barrier ("\_CheckpointConnector.resume_end")                                                                                                                                                                                                                                                | Clears gpu cache and loaded checkpoints. Also waits for all ranks to catch up and clear their own caches.<br><br>Does not seem to save the checkpoint here.<br><br>This brings GPU usage down to ==6.84Gb==                                                                                                                                                                                |
| trainer.py                             | 1012                             | self.\_run_stage()                                                                                                                                                                                                                                                                                                                                                                                                    | Runs the trainer                                                                                                                                                                                                                                                                                                                                                                           |
|                                        | **1043**<br>1050<br>1051<br>1056 | **def \_run_stage(self)**<br>        if self.evaluating:<br>            return self.\_evaluation_loop.run()<br>	    ....<br>		if self.training:<br>            self.fit_loop.run()                                                                                                                                                                                                                                    | Calls eval, prediction and training loops.                                                                                                                                                                                                                                                                                                                                                 |
| fit_loop.py                            | 208                              | self.setup_data()                                                                                                                                                                                                                                                                                                                                                                                                     | For my test run, this exited out in the following line                                                                                                                                                                                                                                                                                                                                     |
|                                        | 226                              | if self.\_combined_loader is not None and not self.\_should_reload_train_dl:<br>    return                                                                                                                                                                                                                                                                                                                            | With pytorch dataloader, this was true and just returned. Information should already be saved in the trainer object.                                                                                                                                                                                                                                                                       |
|                                        |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                            |
| fit_loop.py                            | 455                              | self.epoch_loop.run(self.\_data_fetcher)                                                                                                                                                                                                                                                                                                                                                                              | Called by self.advance() function in fit_loop.py. Calls the epoch loop code. These differ based on training_epoch_loop, or validation_epoch_loop etc.                                                                                                                                                                                                                                      |
| training \_epoch \_loop.py             | 148                              | self.reset()                                                                                                                                                                                                                                                                                                                                                                                                          | Resets the internal state of the loop for a new epoch run. Also handles things like not resetting relevant gradient accumulation info.                                                                                                                                                                                                                                                     |
|                                        | 149                              | self.on_run_start(data_fetcher)                                                                                                                                                                                                                                                                                                                                                                                       | This is what actually grabs your data. For example, here is where the iterator is defined for the data_fetcher.                                                                                                                                                                                                                                                                            |
|                                        | 245<br>246                       | data_fetcher.\_start_profiler = self.\_on_before_fetch<br>data_fetcher.\_stop_profiler = self.\_on_after_fetch                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 152                              | self.advance(data_fetcher)                                                                                                                                                                                                                                                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                            |
| **training \_epoch \_loop.py**         | **269**                          | **def advance(self, data_fetcher)**                                                                                                                                                                                                                                                                                                                                                                                   | **Runs a single training batch**                                                                                                                                                                                                                                                                                                                                                           |
|                                        | 294<br>295<br>                   | if ... (distributed runtime with devices>1)<br>    self.\_broadcast_sigterm_tensor()                                                                                                                                                                                                                                                                                                                                  | Makes a call to broadcast the sigterm tensor accross all gpus if you are in distributed computing.                                                                                                                                                                                                                                                                                         |
|                                        | **254**                          | **def \_broadcast_sigterm_tensor(self)**                                                                                                                                                                                                                                                                                                                                                                              | **Broadcasts sigterm tensor**                                                                                                                                                                                                                                                                                                                                                              |
|                                        | 256                              | sigterm_tensor = torch.tensor(\[1 if getattr(self.trainer, "received_sigterm", False) else 0\], device=self.trainer.strategy.root_device)                                                                                                                                                                                                                                                                             | Defines the tensor flag for sigterm on the root device connected to the strategy                                                                                                                                                                                                                                                                                                           |
|                                        | 260                              | torch.distributed.broadcast(sigterm_tensor, src=0)                                                                                                                                                                                                                                                                                                                                                                    | broadcasts that tensor across all gpus.                                                                                                                                                                                                                                                                                                                                                    |
|                                        | 266                              | if sigterm_tensor.item() == 1: <br>    with contextlib.suppress(Exception): <br>	    torch.distributed.barrier()<br>	raise SIGTERMException()                                                                                                                                                                                                                                                                         | If sigterm is called on any rank, creates a barrier to synchronize all ranks, then calls the exception on all ranks at once.                                                                                                                                                                                                                                                               |
| **training \_epoch \_loop.py**         |                                  | **back into advance function**                                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 299 - 311                        | ...<br>batch = next(data_fetcher)<br>batrch_idx = self.batch_ids + 1<br>## *This is done slightly differently if you are using an iter for your dataloader*                                                                                                                                                                                                                                                           | Gets the next batch of data, check if this is the last batch. If it is, sets the done flag in the data_fetcher (which connects to the profiler's done flag)                                                                                                                                                                                                                                |
|                                        | 314<br>315                       | if not using_dataloader_iter<br>    batch = trainer.precision_plugin.convert_input (batch)                                                                                                                                                                                                                                                                                                                            | Convert model inputs (forward) to the floating point precision type of this plugin                                                                                                                                                                                                                                                                                                         |
|                                        | 344                              | batch_output = self.automatic_optimization.run (trainer.optimizers[0], batch_idx, kwargs)                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                            |
| **loops > optim > automatic.py**       | **163**                          | **def run(self, optimizer, batch_idx, kwargs)**                                                                                                                                                                                                                                                                                                                                                                       | **Called by advance()** Runs closure (train step + backward) together with optimization if necessary                                                                                                                                                                                                                                                                                       |
|                                        | 172                              | closure = self.\_make_closure(kwargs, optimizer, batch_idx)                                                                                                                                                                                                                                                                                                                                                           | Build a closure object that captures the given arguments and runs the training_step function and<br>optionally other functions such as backward and zero_grad.                                                                                                                                                                                                                             |
|                                        | **199**                          | **def \_make_closure(self, kwags, optimizer, batch_idx)**                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 202<br>**207**<br>209            | step_fn = self.\_make_step_fn(kwargs)<br>**def \_make_step_fn(self, kwargs)**<br>        return partial(self.\_training_step, kwargs)                                                                                                                                                                                                                                                                                 | ==Runs the actual training step you defined.==                                                                                                                                                                                                                                                                                                                                             |
|                                        |                                  |                                                                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 192                              | self.\_optimizer_step(batch_idx, closure)                                                                                                                                                                                                                                                                                                                                                                             | ==Fully running this resulted in 11.968Gb added to GPU==                                                                                                                                                                                                                                                                                                                                   |
|                                        | **245**                          | **def \_optimizer_step(self, batch_idx, closure)**                                                                                                                                                                                                                                                                                                                                                                    | **Performs on step through your optimizer**                                                                                                                                                                                                                                                                                                                                                |
|                                        | 261                              | optimizer = trainer.strategy. \_lightning_optimizers[0]                                                                                                                                                                                                                                                                                                                                                               | This is how pytorch lightning retrieves your optimizer. They are stored in the strategy object to be able to work with sharding                                                                                                                                                                                                                                                            |
|                                        | 270                              | call.\_call_lightning_module_hook(trainer, "optimizer_step", trainer.current_epoch, batch_idx, optimizer, closure)                                                                                                                                                                                                                                                                                                    | Calls the hook to run the optimization                                                                                                                                                                                                                                                                                                                                                     |
| **call.py**                            | **154**                          | **def \_call_lightning_module_hook(trainer, hook_name, \*args, pl_module, \*\*kwargs)**                                                                                                                                                                                                                                                                                                                               | **Hook name is parsed to figure out which function should be called using this hook. Following lines will use: hook_name = "optimizer_step"**                                                                                                                                                                                                                                              |
| module.py                              | **1297**<br>1328                 | **def optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure)**<br>    optimizer.step(closure=optimizer_closure)                                                                                                                                                                                                                                                                                         | By default, lightning calls .step and .zero_grad() through this function                                                                                                                                                                                                                                                                                                                   |
| **optimizer.py**                       | **85**                           | **def step(self, closure, \*\*kwargs)**                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 146                              | self.\_on_before_step()                                                                                                                                                                                                                                                                                                                                                                                               | Default: calls a do nothing closure                                                                                                                                                                                                                                                                                                                                                        |
|                                        | 154                              | step_output = self.\_strategy.optimizer_step (self.\_optimizer, closure, \*\*kwargs)                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                            |
| **ddp.py**                             | **254**                          | **def optimizer_step(self, optimizer, closure, model, \*\*kwargs)**                                                                                                                                                                                                                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 270                              | optimizer_output = super().optimizer.step (optimizer, closure, model, \*\*kwargs)                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                            |
| **strategy.py**                        | **220**<br>239                   | **def optimizer_step(...)**<br>    return self.precision_plugin.optimizer_step (optimizer, model, closure, \*\*kwargs)                                                                                                                                                                                                                                                                                                |                                                                                                                                                                                                                                                                                                                                                                                            |
| **precision.py**                       | **114**                          | **def optimizer_step(...)**                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | 122                              | closure = partial(self.\_wrap_closure, model, optimizer, closure)                                                                                                                                                                                                                                                                                                                                                     | Wraps the model/optimizer in another closure. This allows for hooks to inspect gradients before (zero_grad()) or after (gradient clipping) calling optimizer.step().                                                                                                                                                                                                                       |
|                                        | 123                              | return optimizer.step(closure, \*\*kwargs)                                                                                                                                                                                                                                                                                                                                                                            | Calls pytorch's optimizer code for the optimizer you chose to use. ==This is what increased GPU memory to 18.79==. Running this handles all previously defined hooks in the closure (forward pass etc). <br>- hook_name="on_before_backward" = 0.04Gb<br>- hook_name="on_after_backward" = 2.97Gb<br>                                                                                      |
| training \_epoch \_loop.py             | 352                              | self.update_lr_schedulers("step", update_plateau_schedulers=False)                                                                                                                                                                                                                                                                                                                                                    | Updates learning rate schedulers                                                                                                                                                                                                                                                                                                                                                           |
|                                        | 365                              | trainer.\_logger_connector.on_batch_end()                                                                                                                                                                                                                                                                                                                                                                             | Updates loggers and progress bar (epochs/iters completed)                                                                                                                                                                                                                                                                                                                                  |
|                                        | 367                              | self.batch_progress.increment_completed()                                                                                                                                                                                                                                                                                                                                                                             | Updates counts of completed batches. This also updates                                                                                                                                                                                                                                                                                                                                     |
|                                        | 372                              | trainer.\_logger_connector. update_train_step_metrics()                                                                                                                                                                                                                                                                                                                                                               | saves metrics to loggers and progress bar                                                                                                                                                                                                                                                                                                                                                  |
|                                        | 153                              | self.on_advance_end()                                                                                                                                                                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        | **374**                          | **def on_advance_end(self, data_fetcher)**                                                                                                                                                                                                                                                                                                                                                                            | **Runs validation if needed. Handles other logging and lr_scheduler updates if needed (for plateau schedulers).**                                                                                                                                                                                                                                                                          |
|                                        |                                  | if should_check_val:                                                                                                                                                                                                                                                                                                                                                                                                  |                                                                                                                                                                                                                                                                                                                                                                                            |
|                                        |                                  | self.trainer.validating = true                                                                                                                                                                                                                                                                                                                                                                                        | Converts to eval mode                                                                                                                                                                                                                                                                                                                                                                      |
|                                        |                                  | first_loop_iter = self.trainer.\_logger_connector. \_first_loop_iter                                                                                                                                                                                                                                                                                                                                                  | Resets eval loop in case it was run during training loop.                                                                                                                                                                                                                                                                                                                                  |
|                                        |                                  | call.\_call_lightning_module_hook(self.trainer, "on_validation_model_zero_grad")                                                                                                                                                                                                                                                                                                                                      | Zero-grads the optmizer                                                                                                                                                                                                                                                                                                                                                                    |
|                                        |                                  | self.val_loop.run()                                                                                                                                                                                                                                                                                                                                                                                                   | Runs validation loop. This is similar to training loop except it does not run the optimizer                                                                                                                                                                                                                                                                                                |
|                                        |                                  | self.trainer.training = True                                                                                                                                                                                                                                                                                                                                                                                          | Resets trainer to .train()                                                                                                                                                                                                                                                                                                                                                                 |
|                                        |                                  | self.trainer.\_logger_connector.\_first_loop_iter = first_loop_iter                                                                                                                                                                                                                                                                                                                                                   | Resets validation progress to what it was before                                                                                                                                                                                                                                                                                                                                           |

### Some important functionality:

| Function | File it goes to        | End-Code                                                                                                                                                                             | What it does                               |
| -------- | ---------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------ |
| teardown | accelerators > cuda.py | if hasattr(torch.\_C, "\_cuda_clearCublas Workspaces"):<br>    # github.com/pytorch/pytorch/issues/95668<br>    torch.\_C.\_cuda_clearCublasWorkspaces()<br>torch.cuda.empty_cache() | Clears current workspace. Clears the cache |
|          |                        |                                                                                                                                                                                      |                                            |
